import logging
import aiohttp
import asyncio
import os
import re
import sys
import tarfile
import io
import gc  # å¯¼å…¥åƒåœ¾å›æ”¶æ¨¡å—
from functools import wraps
from urllib.parse import urlparse
from telegram import Update
from telegram.constants import ParseMode
from telegram.ext import ApplicationBuilder, CommandHandler, ContextTypes, filters
from telegram.helpers import escape_markdown
import nest_asyncio
from dotenv import load_dotenv
from datetime import datetime, timedelta

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# å…è®¸åµŒå¥—äº‹ä»¶å¾ªç¯
nest_asyncio.apply()

# å¯ç”¨æ—¥å¿—è®°å½•
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# è®¾ç½® httpx æ—¥å¿—çº§åˆ«ä¸º WARNINGï¼Œå±è”½ INFO çº§åˆ«æ—¥å¿—
logging.getLogger("httpx").setLevel(logging.WARNING)

# æœºå™¨äºº Tokenï¼ˆè¯·ç¡®ä¿å®‰å…¨å­˜å‚¨ï¼Œä¸è¦ç¡¬ç¼–ç ï¼‰
BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')  # ç¡®ä¿å·²è®¾ç½®ç¯å¢ƒå˜é‡

# ç®¡ç†å‘˜ç”¨æˆ·IDåˆ—è¡¨ï¼Œä»ç¯å¢ƒå˜é‡è¯»å–
ADMIN_USER_IDS = os.getenv('ADMIN_USER_IDS', '')
ADMIN_IDS = [int(uid.strip()) for uid in ADMIN_USER_IDS.split(',') if uid.strip().isdigit()]

# å‹ç¼©åŒ…çš„ä¸‹è½½é“¾æ¥
ARCHIVE_URL = 'https://github.com/Cats-Team/upstream-artifacts/raw/refs/heads/main/archive.tar.gz'

# å­˜å‚¨ URL å†…å®¹ï¼Œç»“æ„ä¸º {ç±»åˆ«åç§°: {url: content, ...}, ...}
url_contents = {}

# å®šä¹‰æœ‰æ•ˆçš„ç±»åˆ«ï¼ŒåŒ…æ‹¬ 'all'
VALID_CATEGORIES = ['content', 'dns', 'all']

# æœ€å¤§é‡è¯•æ¬¡æ•°
MAX_RETRIES = 5
# é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
RETRY_INTERVAL = 5

# æœ€å¤§æ¶ˆæ¯é•¿åº¦
MAX_MESSAGE_LENGTH = 4096
# æœ€å¤§æ¶ˆæ¯æ•°é‡
MAX_MESSAGES = 3

# å…¨å±€å˜é‡
initial_load_time = None  # é¦–æ¬¡æˆåŠŸåŠ è½½è§„åˆ™çš„æ—¶é—´
last_update_time = None
recent_user_ids = {}

# å°è¯•å¯¼å…¥ psutilï¼Œå¦‚æœæœªå®‰è£…åˆ™è®¾ç½®ä¸º None
try:
    import psutil
except ImportError:
    psutil = None
    logger.error("psutil æ¨¡å—æœªå®‰è£…ï¼Œ/sysinf å‘½ä»¤å°†ä¸å¯ç”¨ã€‚")

def log_user_command(func):
    """
    è£…é¥°å™¨ï¼šè®°å½•ç”¨æˆ·å‘½ä»¤çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç”¨æˆ·IDã€ç¾¤ç»„IDå’Œå®Œæ•´æ¶ˆæ¯ã€‚
    å¹¶æ›´æ–°æœ€è¿‘ä½¿ç”¨è¿‡çš„ç”¨æˆ·IDåˆ—è¡¨ã€‚
    """
    @wraps(func)
    async def wrapper(update: Update, context: ContextTypes.DEFAULT_TYPE):
        global recent_user_ids
        user = update.effective_user
        chat = update.effective_chat
        message = update.message.text if update.message else "æ— æ¶ˆæ¯å†…å®¹"
        user_id = user.id if user else "æœªçŸ¥ç”¨æˆ·ID"
        chat_id = chat.id if chat else "æœªçŸ¥ç¾¤ç»„ID"
        logger.info(f"ç¾¤ç»„ID: {chat_id}, ç”¨æˆ·ID: {user_id} å‘é€äº†å‘½ä»¤: {message}")

        # æ›´æ–°æœ€è¿‘ä½¿ç”¨è¿‡çš„ç”¨æˆ·IDåˆ—è¡¨
        if user_id != "æœªçŸ¥ç”¨æˆ·ID":
            now = datetime.now()
            recent_user_ids[user_id] = now
            # æ¸…ç†ä¸€å°æ—¶å‰çš„ç”¨æˆ·ID
            cutoff_time = now - timedelta(hours=1)
            uids_to_remove = [uid for uid, t in recent_user_ids.items() if t < cutoff_time]
            for uid in uids_to_remove:
                del recent_user_ids[uid]

        return await func(update, context)
    return wrapper

def is_user_admin(user_id):
    """
    æ£€æŸ¥ç”¨æˆ·æ˜¯å¦ä¸ºç®¡ç†å‘˜ã€‚
    """
    return user_id in ADMIN_IDS

async def download_archive(session: aiohttp.ClientSession, url: str) -> bytes:
    """
    å¼‚æ­¥ä¸‹è½½å‹ç¼©åŒ…å†…å®¹ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶ã€‚
    å¦‚æœè¿ç»­äº”æ¬¡å¤±è´¥ï¼Œç¨‹åºå°†ç»ˆæ­¢ã€‚
    """
    retries = 0
    while retries < MAX_RETRIES:
        try:
            async with session.get(url, timeout=15) as response:
                response.raise_for_status()
                data = await response.read()
                logger.info(f"æˆåŠŸä¸‹è½½å‹ç¼©åŒ…: {url}")
                return data
        except Exception as e:
            retries += 1
            logger.error(f"ä¸‹è½½ {url} æ—¶å‡ºé”™ (å°è¯• {retries}/{MAX_RETRIES})ï¼š{e}")
            if retries < MAX_RETRIES:
                logger.info(f"ç­‰å¾… {RETRY_INTERVAL} ç§’åé‡è¯• {url}...")
                await asyncio.sleep(RETRY_INTERVAL)
            else:
                logger.critical(f"è¿ç»­ {MAX_RETRIES} æ¬¡å¤±è´¥ï¼Œæ— æ³•ä¸‹è½½å‹ç¼©åŒ… {url}ã€‚")
                raise Exception(f"æ— æ³•ä¸‹è½½å‹ç¼©åŒ… {url}")
    return b""

async def download_and_parse_archive(archive_url: str):
    """
    ä¸‹è½½å‹ç¼©åŒ…å¹¶è§£æå…¶ä¸­çš„æ–‡ä»¶ï¼Œå°†å†…å®¹å­˜å‚¨åˆ° `url_contents` ä¸­ã€‚
    """
    global url_contents, last_update_time, initial_load_time
    async with aiohttp.ClientSession() as session:
        archive_data = await download_archive(session, archive_url)
        if not archive_data:
            logger.error("æœªèƒ½ä¸‹è½½å‹ç¼©åŒ…ã€‚")
            raise Exception("æœªèƒ½ä¸‹è½½å‹ç¼©åŒ…")

        try:
            # ä½¿ç”¨ BytesIO å’Œ with è¯­å¥ç¡®ä¿åŠæ—¶é‡Šæ”¾å†…å­˜
            with io.BytesIO(archive_data) as archive_io:
                with tarfile.open(fileobj=archive_io, mode='r:gz') as tar:
                    members = tar.getmembers()
                    temp_url_contents = {}
                    for member in members:
                        if member.isfile():
                            file_path = member.name
                            # æœŸæœ›çš„æ–‡ä»¶è·¯å¾„æ ¼å¼ï¼š./tmp/content/filename.txt æˆ– ./tmp/dns/filename.txt
                            parts = file_path.split('/')
                            if len(parts) >= 3:
                                category = parts[2]  # 'content' æˆ– 'dns'
                                filename = parts[-1]
                                # è¯»å–æ–‡ä»¶å†…å®¹
                                f = tar.extractfile(member)
                                if f:
                                    lines = f.read().decode('utf-8', errors='ignore').splitlines()
                                    if not lines:
                                        continue
                                    # ç¬¬ä¸€è¡Œæ˜¯ URL
                                    first_line = lines[0].strip()
                                    url_match = re.match(r'^! url:\s*(\S+)', first_line, re.IGNORECASE)
                                    if url_match:
                                        url = url_match.group(1)
                                        content = '\n'.join(lines[1:])  # å‰©ä½™å†…å®¹
                                        if category not in temp_url_contents:
                                            temp_url_contents[category] = {}
                                        temp_url_contents[category][url] = content
                                        logger.info(f"åŠ è½½ {category} ç±»åˆ«çš„ URL: {url}")
                                    else:
                                        logger.warning(f"æ–‡ä»¶ {file_path} çš„ç¬¬ä¸€è¡Œæœªæ‰¾åˆ° URL ä¿¡æ¯ã€‚")
                    # æ‰‹åŠ¨åˆ é™¤æ—§çš„ url_contents æ•°æ®å¹¶è°ƒç”¨åƒåœ¾å›æ”¶
                    if url_contents:
                        del url_contents
                        gc.collect()
                    # æ›´æ–°å…¨å±€ url_contents
                    url_contents = temp_url_contents
                    # æ›´æ–°æœ€åæ›´æ–°æ—¶é—´
                    last_update_time = datetime.now()
                    # å¦‚æœ initial_load_time å°šæœªè®¾ç½®ï¼Œåˆ™è®¾ç½®ä¸ºå½“å‰æ—¶é—´
                    if initial_load_time is None:
                        initial_load_time = last_update_time
        except tarfile.TarError as e:
            logger.error(f"è§£å‹ç¼©åŒ…æ—¶å‡ºé”™: {e}")
            raise e  # é‡æ–°æŠ›å‡ºå¼‚å¸¸
        finally:
            # æ‰‹åŠ¨åˆ é™¤ä¸å†éœ€è¦çš„å˜é‡å¹¶è°ƒç”¨åƒåœ¾å›æ”¶
            del archive_data
            gc.collect()

def split_message(message: str) -> list:
    """
    å°†é•¿æ¶ˆæ¯æ‹†åˆ†ä¸ºå¤šä¸ªä¸è¶…è¿‡ MAX_MESSAGE_LENGTH çš„éƒ¨åˆ†ã€‚
    æœ€å¤šè¿”å› MAX_MESSAGES æ¡æ¶ˆæ¯ã€‚
    æ‹†åˆ†æ—¶ä»…åœ¨åŒæ¢è¡Œç¬¦å¤„è¿›è¡Œï¼Œé¿å…åœ¨ä»£ç å—ä¸­é—´æ‹†åˆ†ã€‚
    """
    messages = []
    parts = message.split('\n\n')
    current_message = ""
    for part in parts:
        # è®¡ç®—åŠ å…¥å½“å‰éƒ¨åˆ†åçš„é•¿åº¦
        if len(current_message) + len(part) + 2 > MAX_MESSAGE_LENGTH:
            if current_message:
                messages.append(current_message)
                if len(messages) >= MAX_MESSAGES:
                    messages.append(f"*æ³¨æ„ï¼š* æœç´¢ç»“æœè¿‡å¤šï¼Œä»…æ˜¾ç¤ºå‰ {MAX_MESSAGES} æ¡æ¶ˆæ¯ã€‚")
                    break
            current_message = part
        else:
            if current_message:
                current_message += "\n\n" + part
            else:
                current_message = part
    if current_message and len(messages) < MAX_MESSAGES:
        messages.append(current_message)
    return messages

@log_user_command
async def send_help_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    å‘é€å¸®åŠ©ä¿¡æ¯ã€‚
    """
    help_text = (
        "ğŸ“š *å¸®åŠ©ä¿¡æ¯*\n\n"
        "*å‘½ä»¤åˆ—è¡¨ï¼š*\n"
        "/start \\- å¯åŠ¨æœºå™¨äººå¹¶æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯\n"
        "/search */ç±»åˆ«* `å…³é”®è¯` \\- æœç´¢æŒ‡å®šç±»åˆ«ä¸­åŒ…å«å…³é”®è¯çš„è¿‡æ»¤è§„åˆ™ URL\n"
        "/regex */ç±»åˆ«* `æ­£åˆ™è¡¨è¾¾å¼` \\- ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢æŒ‡å®šç±»åˆ«ä¸­çš„è¿‡æ»¤è§„åˆ™ URL\n"
        "/help \\- æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯\n"
        "/info \\- æ˜¾ç¤ºæœºå™¨äººå’Œè§„åˆ™æ›´æ–°çš„ç›¸å…³ä¿¡æ¯\n\n"
        "*å¯ç”¨ç±»åˆ«ï¼š* /content, /dns, /all\n\n"
        "ğŸ“Œ ç¤ºä¾‹ï¼š\n"
        "`/search /dns ||iqiyi.com^`\n"
        "`/regex /content ^iqiyi.com$`\n"
        "\n"
        "*æ³¨æ„ï¼š* ç±»åˆ«å‚æ•°æ˜¯å¯é€‰çš„ï¼Œé»˜è®¤ä¸º /allã€‚"
        "\n\n*V1\\.3\\.1*"
    )
    if update.message:
        messages = split_message(help_text)
        for msg in messages:
            await update.message.reply_text(
                msg,
                parse_mode=ParseMode.MARKDOWN_V2,
                disable_web_page_preview=True
            )
    else:
        logger.warning("æ— æ³•å‘é€å¸®åŠ©ä¿¡æ¯ï¼Œå› ä¸º update.message ä¸º None")

@log_user_command
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    å¤„ç† /start å‘½ä»¤ï¼Œå‘é€å¸®åŠ©ä¿¡æ¯ã€‚
    """
    await send_help_message(update, context)

@log_user_command
async def search(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    å¤„ç† /search å‘½ä»¤ï¼Œæ ¹æ®ç±»åˆ«å’Œå…³é”®è¯æœç´¢åŒ…å«è¯¥å…³é”®è¯çš„è¿‡æ»¤è§„åˆ™ URLã€‚
    å¹¶æ˜¾ç¤ºåŒ¹é…è¡Œçš„è¡Œå·åŠå†…å®¹ã€‚
    """
    if len(context.args) < 1:
        if update.message:
            await update.message.reply_text('ç”¨æ³•ï¼š`/search /ç±»åˆ« å…³é”®è¯`ã€‚ä¾‹å¦‚ï¼š`/search /dns \\|\\|iqiyi\\.com^`\nç±»åˆ«å‚æ•°æ˜¯å¯é€‰çš„ï¼Œé»˜è®¤ä¸º /allã€‚', parse_mode=ParseMode.MARKDOWN_V2)
        else:
            logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")
        return

    # é»˜è®¤ç±»åˆ«
    category = 'all'

    # æ£€æŸ¥ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å¦ä¸ºç±»åˆ«
    if context.args[0].startswith('/'):
        category_arg = context.args[0]
        category = category_arg[1:].lower()
        if category not in VALID_CATEGORIES:
            if update.message:
                await update.message.reply_text(f'æ— æ•ˆçš„ç±»åˆ« "{escape_markdown(category_arg, version=2)}"ã€‚å¯ç”¨ç±»åˆ«ï¼š/content, /dns, /all', parse_mode=ParseMode.MARKDOWN_V2)
            else:
                logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")
            return
        # æå–å…³é”®è¯
        keyword = ' '.join(context.args[1:]).lower()
    else:
        # æœªæŒ‡å®šç±»åˆ«ï¼Œé»˜è®¤ä½¿ç”¨ 'all'
        keyword = ' '.join(context.args).lower()

    if not keyword:
        if update.message:
            await update.message.reply_text('è¯·æä¾›è¦æœç´¢çš„å…³é”®è¯ã€‚ä¾‹å¦‚ï¼š`/search \\|\\|iqiyi\\.com^`', parse_mode=ParseMode.MARKDOWN_V2)
        else:
            logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")
        return

    found_results = {}

    # å¦‚æœç±»åˆ«æ˜¯ 'all', éå†æ‰€æœ‰ç±»åˆ«
    categories_to_search = VALID_CATEGORIES[:-1] if category == 'all' else [category]

    for cat in categories_to_search:
        for url, content in url_contents.get(cat, {}).items():
            if not content:
                continue
            lines = content.splitlines()
            matching_lines = []
            for idx, line in enumerate(lines, start=1):
                if keyword in line.lower():
                    matching_lines.append((idx, line.strip()))
            if matching_lines:
                if cat not in found_results:
                    found_results[cat] = {}
                found_results[cat][url] = matching_lines

    if found_results:
        response = f"*æœç´¢ç±»åˆ«ï¼š*/{escape_markdown(category, version=2)}\n*å…³é”®è¯ï¼š*`{escape_markdown(keyword, version=2)}`\n\n"
        for cat, urls in found_results.items():
            response += f"*{escape_markdown(cat, version=2)}*:\n\n"
            for url, matches in urls.items():
                parsed_url = urlparse(url)
                filename = os.path.basename(parsed_url.path)
                escaped_filename = escape_markdown(filename, version=2)
                escaped_url = escape_markdown(url, version=2)
                response += f"â€¢ [{escaped_filename}]({escaped_url})\n"
                for line_num, line_content in matches:
                    line_content_escaped = escape_markdown(line_content, version=2)
                    response += f"> *{line_num}:*\n> `{line_content_escaped}`\n\n"
            response += "\n"  # æ·»åŠ ç©ºè¡Œä»¥åˆ†éš”ä¸åŒç±»åˆ«
    else:
        response = f"åœ¨ç±»åˆ« /{escape_markdown(category, version=2)} ä¸­æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ '`{escape_markdown(keyword, version=2)}`' çš„è¿‡æ»¤è§„åˆ™ URLã€‚"

    # æ‹†åˆ†æ¶ˆæ¯
    messages = split_message(response)

    if messages:
        for msg in messages:
            if update.message:
                await update.message.reply_text(
                    msg,
                    parse_mode=ParseMode.MARKDOWN_V2,
                    disable_web_page_preview=True
                )
            else:
                logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")

@log_user_command
async def regex_search(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    å¤„ç† /regex å‘½ä»¤ï¼Œæ ¹æ®ç±»åˆ«å’Œæ­£åˆ™è¡¨è¾¾å¼æœç´¢åŒ¹é…çš„è¿‡æ»¤è§„åˆ™ URLã€‚
    å¹¶æ˜¾ç¤ºåŒ¹é…è¡Œçš„è¡Œå·åŠå†…å®¹ã€‚
    """
    if len(context.args) < 1:
        if update.message:
            await update.message.reply_text('ç”¨æ³•ï¼š`/regex /ç±»åˆ« æ­£åˆ™è¡¨è¾¾å¼`ã€‚ä¾‹å¦‚ï¼š`/regex /content ^iqiyi\\.com$`\nç±»åˆ«å‚æ•°æ˜¯å¯é€‰çš„ï¼Œé»˜è®¤ä¸º /allã€‚', parse_mode=ParseMode.MARKDOWN_V2)
        else:
            logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")
        return

    # é»˜è®¤ç±»åˆ«
    category = 'all'

    # æ£€æŸ¥ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å¦ä¸ºç±»åˆ«
    if context.args[0].startswith('/'):
        category_arg = context.args[0]
        category = category_arg[1:].lower()
        if category not in VALID_CATEGORIES:
            if update.message:
                await update.message.reply_text(f'æ— æ•ˆçš„ç±»åˆ« "{escape_markdown(category_arg, version=2)}"ã€‚å¯ç”¨ç±»åˆ«ï¼š/content, /dns, /all', parse_mode=ParseMode.MARKDOWN_V2)
            else:
                logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")
            return
        # æå–æ­£åˆ™è¡¨è¾¾å¼
        pattern = ' '.join(context.args[1:])
    else:
        # æœªæŒ‡å®šç±»åˆ«ï¼Œé»˜è®¤ä½¿ç”¨ 'all'
        pattern = ' '.join(context.args)

    if not pattern:
        if update.message:
            await update.message.reply_text('è¯·æä¾›è¦æœç´¢çš„æ­£åˆ™è¡¨è¾¾å¼ã€‚ä¾‹å¦‚ï¼š`/regex ^iqiyi\\.com$`', parse_mode=ParseMode.MARKDOWN_V2)
        else:
            logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")
        return

    try:
        regex = re.compile(pattern, re.IGNORECASE)
    except re.error as e:
        if update.message:
            escaped_error = escape_markdown(str(e), version=2)
            await update.message.reply_text(f'æ— æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼ï¼š{escaped_error}', parse_mode=ParseMode.MARKDOWN_V2)
        else:
            logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")
        return

    found_results = {}

    # å¦‚æœç±»åˆ«æ˜¯ 'all', éå†æ‰€æœ‰ç±»åˆ«
    categories_to_search = VALID_CATEGORIES[:-1] if category == 'all' else [category]

    for cat in categories_to_search:
        for url, content in url_contents.get(cat, {}).items():
            if not content:
                continue
            lines = content.splitlines()
            matching_lines = []
            for idx, line in enumerate(lines, start=1):
                if regex.search(line):
                    matching_lines.append((idx, line.strip()))
            if matching_lines:
                if cat not in found_results:
                    found_results[cat] = {}
                found_results[cat][url] = matching_lines

    if found_results:
        escaped_pattern = escape_markdown(pattern, version=2)
        response = f"*æ­£åˆ™è¡¨è¾¾å¼æœç´¢ç±»åˆ«ï¼š*/{escape_markdown(category, version=2)}\n*æ­£åˆ™è¡¨è¾¾å¼ï¼š*`{escaped_pattern}`\n\n"
        for cat, urls in found_results.items():
            response += f"*{escape_markdown(cat, version=2)}*:\n\n"
            for url, matches in urls.items():
                parsed_url = urlparse(url)
                filename = os.path.basename(parsed_url.path)
                escaped_filename = escape_markdown(filename, version=2)
                escaped_url = escape_markdown(url, version=2)
                response += f"â€¢ [{escaped_filename}]({escaped_url})\n"
                for line_num, line_content in matches:
                    line_content_escaped = escape_markdown(line_content, version=2)
                    response += f"> *{line_num}:*\n> `{line_content_escaped}`\n\n"
            response += "\n"  # æ·»åŠ ç©ºè¡Œä»¥åˆ†éš”ä¸åŒç±»åˆ«
    else:
        escaped_pattern = escape_markdown(pattern, version=2)
        response = f"åœ¨ç±»åˆ« /{escape_markdown(category, version=2)} ä¸­æœªæ‰¾åˆ°ç¬¦åˆæ­£åˆ™è¡¨è¾¾å¼ '`{escaped_pattern}`' çš„è¿‡æ»¤è§„åˆ™ URLã€‚"

    # æ‹†åˆ†æ¶ˆæ¯
    messages = split_message(response)

    if messages:
        for msg in messages:
            if update.message:
                await update.message.reply_text(
                    msg,
                    parse_mode=ParseMode.MARKDOWN_V2,
                    disable_web_page_preview=True
                )
            else:
                logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")

@log_user_command
async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    å¤„ç† /help å‘½ä»¤ï¼Œå‘é€å¸®åŠ©ä¿¡æ¯ã€‚
    """
    await send_help_message(update, context)

@log_user_command
async def xinfo(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    å¤„ç† /xinfo å’Œ /info å‘½ä»¤ï¼Œæ˜¾ç¤ºæœºå™¨äººä¿¡æ¯å’Œç»Ÿè®¡æ•°æ®ã€‚
    """
    global initial_load_time, last_update_time, recent_user_ids

    now = datetime.now()

    # è®¡ç®—ä¸‹æ¬¡æ›´æ–°æ—¶é—´
    if last_update_time:
        next_update_time = last_update_time + timedelta(hours=1)
    else:
        next_update_time = 'æœªçŸ¥'

    # æ ¼å¼åŒ–æ—¶é—´
    initial_load_str = initial_load_time.strftime('%Y-%m-%d %H:%M:%S') if initial_load_time else 'æœªçŸ¥'
    last_update_str = last_update_time.strftime('%Y-%m-%d %H:%M:%S') if last_update_time else 'æœªçŸ¥'
    next_update_str = next_update_time.strftime('%Y-%m-%d %H:%M:%S') if isinstance(next_update_time, datetime) else 'æœªçŸ¥'

    # è·å–æ¯ä¸ªç±»åˆ«çš„æ–‡ä»¶æ•°é‡
    category_counts = {}
    for category in VALID_CATEGORIES[:-1]:  # Exclude 'all'
        category_counts[category] = len(url_contents.get(category, {}))

    # æ ¼å¼åŒ–ç”¨æˆ·ID
    def format_user_id(user_id):
        uid_str = str(user_id)
        if len(uid_str) <= 4:
            masked_uid = uid_str  # ä¸é®æ©é•¿åº¦å°äºç­‰äº4çš„ID
        else:
            masked_uid = uid_str[:2] + '*' * (len(uid_str) - 4) + uid_str[-2:]
        # æ£€æŸ¥æ˜¯å¦ä¸ºç®¡ç†å‘˜
        if is_user_admin(user_id):
            masked_uid += ' #'  # æ·»åŠ ç®¡ç†å‘˜æ ‡è®°
        return masked_uid

    # è·å–ä¸€å°æ—¶å†…ä½¿ç”¨è¿‡çš„ç”¨æˆ·IDåˆ—è¡¨
    user_ids_within_hour = [uid for uid, t in recent_user_ids.items() if t >= now - timedelta(hours=1)]
    # æ ¼å¼åŒ–å¹¶å¤„ç†ç”¨æˆ·ID
    formatted_user_ids = []
    for uid in user_ids_within_hour:
        uid_str = format_user_id(uid)
        # è½¬ä¹‰åå¼•å·
        uid_str_escaped = uid_str.replace('`', '\\`')
        # ä½¿ç”¨ç­‰å®½å­—ä½“æ˜¾ç¤º
        uid_str_monospace = f'`{uid_str_escaped}`'
        formatted_user_ids.append(uid_str_monospace)

    # ç»„è£…å“åº”æ¶ˆæ¯
    response = (
        f"*æœºå™¨äººä¿¡æ¯ï¼š*\n"
        f"å¯åŠ¨æ—¶é—´ï¼š{escape_markdown(initial_load_str, version=2)}\n"
        f"ä¸Šæ¬¡è§„åˆ™æ›´æ–°æ—¶é—´ï¼š{escape_markdown(last_update_str, version=2)}\n"
        f"ä¸‹æ¬¡æ›´æ–°æ—¶é—´ï¼š{escape_markdown(next_update_str, version=2)}\n"
        f"\n"
        f"*è§„åˆ™æ–‡ä»¶æ•°é‡ï¼š*\n"
    )
    for category, count in category_counts.items():
        response += f"`{escape_markdown(category, version=2)}ï¼š{count}`\n"

    response += "\n"
    response += "*ä¸€å°æ—¶å†…ä½¿ç”¨è¿‡çš„ç”¨æˆ·IDåˆ—è¡¨ï¼š*\n"
    if formatted_user_ids:
        response += '\n'.join(formatted_user_ids)
    else:
        response += 'æ— '

    # å‘é€å“åº”æ¶ˆæ¯
    if update.message:
        await update.message.reply_text(
            response,
            parse_mode=ParseMode.MARKDOWN_V2,
            disable_web_page_preview=True
        )
    else:
        logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")

@log_user_command
async def fetchnow(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    å¤„ç† /fetchnow å‘½ä»¤ï¼Œå¼ºåˆ¶é‡æ–°åŠ è½½è§„åˆ™ï¼Œä½†åªæœ‰ç®¡ç†å‘˜ç”¨æˆ·æ‰èƒ½ä½¿ç”¨ã€‚
    """
    user = update.effective_user
    user_id = user.id if user else None
    if not user_id or not is_user_admin(user_id):
        if update.message:
            await update.message.reply_text(
                "æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤å‘½ä»¤ã€‚",
                parse_mode=ParseMode.MARKDOWN_V2,
                disable_web_page_preview=True
            )
        else:
            logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")
        return

    # é‡æ–°åŠ è½½è§„åˆ™
    try:
        await download_and_parse_archive(ARCHIVE_URL)
        if update.message:
            await update.message.reply_text(
                "è§„åˆ™å·²æˆåŠŸé‡æ–°åŠ è½½ã€‚",
                parse_mode=ParseMode.MARKDOWN_V2,
                disable_web_page_preview=True
            )
        else:
            logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")
    except Exception as e:
        logger.error(f"é‡æ–°åŠ è½½è§„åˆ™æ—¶å‡ºé”™ï¼š{e}")
        if update.message:
            await update.message.reply_text(
                "æŠ±æ­‰ï¼Œé‡æ–°åŠ è½½è§„åˆ™æ—¶å‘ç”Ÿé”™è¯¯ã€‚",
                parse_mode=ParseMode.MARKDOWN_V2,
                disable_web_page_preview=True
            )
        else:
            logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")

@log_user_command
async def killnow(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    å¤„ç† /killnow å‘½ä»¤ï¼Œç«‹å³é€€å‡ºæœºå™¨äººç¨‹åºï¼Œä½†åªæœ‰ç®¡ç†å‘˜ç”¨æˆ·æ‰èƒ½ä½¿ç”¨ã€‚
    """
    user = update.effective_user
    user_id = user.id if user else None
    if not user_id or not is_user_admin(user_id):
        if update.message:
            await update.message.reply_text(
                "æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤å‘½ä»¤ã€‚",
                parse_mode=ParseMode.MARKDOWN_V2,
                disable_web_page_preview=True
            )
        else:
            logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")
        return

    if update.message:
        await update.message.reply_text(
            "æœºå™¨äººå³å°†å…³é—­ã€‚",
            parse_mode=ParseMode.MARKDOWN_V2,
            disable_web_page_preview=True
        )
    else:
        logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")
    # åœæ­¢åº”ç”¨ç¨‹åº
    await context.application.stop()
    sys.exit(0)

@log_user_command
async def sysinf(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    å¤„ç† /sysinf å‘½ä»¤ï¼Œæ˜¾ç¤ºå½“å‰Linux VPSç³»ç»Ÿä¿¡æ¯ï¼Œåªæœ‰ç®¡ç†å‘˜ç”¨æˆ·æ‰èƒ½ä½¿ç”¨ã€‚
    """
    user = update.effective_user
    user_id = user.id if user else None
    if not user_id or not is_user_admin(user_id):
        if update.message:
            await update.message.reply_text(
                "æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤å‘½ä»¤ã€‚",
                parse_mode=ParseMode.MARKDOWN_V2,
                disable_web_page_preview=True
            )
        else:
            logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")
        return

    if psutil is None:
        if update.message:
            await update.message.reply_text(
                "psutil æ¨¡å—æœªå®‰è£…ï¼Œæ— æ³•è·å–ç³»ç»Ÿä¿¡æ¯ã€‚",
                parse_mode=ParseMode.MARKDOWN_V2,
                disable_web_page_preview=True
            )
        else:
            logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")
        return

    # è·å–ç³»ç»Ÿä¿¡æ¯
    mem = psutil.virtual_memory()
    swap = psutil.swap_memory()
    cpu_percent = psutil.cpu_percent(interval=1)
    disk = psutil.disk_usage('/')

    # å¯¹å˜é‡è¿›è¡Œè½¬ä¹‰
    cpu_percent_escaped = escape_markdown(str(cpu_percent), version=2)
    mem_used_escaped = escape_markdown(psutil._common.bytes2human(mem.used), version=2)
    mem_total_escaped = escape_markdown(psutil._common.bytes2human(mem.total), version=2)
    mem_percent_escaped = escape_markdown(str(mem.percent), version=2)
    swap_used_escaped = escape_markdown(psutil._common.bytes2human(swap.used), version=2)
    swap_total_escaped = escape_markdown(psutil._common.bytes2human(swap.total), version=2)
    swap_percent_escaped = escape_markdown(str(swap.percent), version=2)
    disk_used_escaped = escape_markdown(psutil._common.bytes2human(disk.used), version=2)
    disk_total_escaped = escape_markdown(psutil._common.bytes2human(disk.total), version=2)
    disk_percent_escaped = escape_markdown(str(disk.percent), version=2)

    # æ„å»ºå“åº”æ¶ˆæ¯ï¼Œæ‰‹åŠ¨è½¬ä¹‰é™æ€æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦
    response = (
        f"*ç³»ç»Ÿä¿¡æ¯ï¼š*\n"
        f"CPUä½¿ç”¨ç‡ï¼š{cpu_percent_escaped}%\n"
        f"å†…å­˜ä½¿ç”¨ï¼š{mem_used_escaped} / {mem_total_escaped} "
        f"\\({mem_percent_escaped}%\\)\n"
        f"äº¤æ¢åŒºä½¿ç”¨ï¼š{swap_used_escaped} / {swap_total_escaped} "
        f"\\({swap_percent_escaped}%\\)\n"
        f"ç£ç›˜ä½¿ç”¨ï¼š{disk_used_escaped} / {disk_total_escaped} "
        f"\\({disk_percent_escaped}%\\)\n"
    )

    # å‘é€å“åº”æ¶ˆæ¯
    if update.message:
        await update.message.reply_text(
            response,
            parse_mode=ParseMode.MARKDOWN_V2,
            disable_web_page_preview=True
        )
    else:
        logger.warning("æ— æ³•å‘é€æ¶ˆæ¯ï¼Œå› ä¸º update.message ä¸º None")

async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    å…¨å±€é”™è¯¯å¤„ç†å™¨ï¼Œè®°å½•é”™è¯¯å¹¶å‘ç”¨æˆ·å‘é€å‹å¥½æç¤ºã€‚
    """
    logger.error(msg="Exception while handling an update:", exc_info=context.error)
    if isinstance(update, Update) and update.effective_message:
        try:
            await update.effective_message.reply_text(
                "æŠ±æ­‰ï¼Œå‘ç”Ÿäº†ä¸€ä¸ªé”™è¯¯ã€‚è¯·ç¨åå†è¯•ã€‚",
                parse_mode=ParseMode.MARKDOWN_V2,
                disable_web_page_preview=True
            )
        except Exception as e:
            logger.error(f"æ— æ³•å‘é€é”™è¯¯æ¶ˆæ¯: {e}")

async def periodic_update(archive_url: str):
    """
    å®šæœŸæ›´æ–°å‹ç¼©åŒ…å†…å®¹çš„ä»»åŠ¡ï¼Œæ¯1å°æ—¶æ‰§è¡Œä¸€æ¬¡ã€‚
    """
    while True:
        await asyncio.sleep(60 * 60)  # ç­‰å¾…1å°æ—¶
        logger.info("å¼€å§‹å®šæœŸæ›´æ–°å‹ç¼©åŒ…...")
        try:
            await download_and_parse_archive(archive_url)
            logger.info("å®šæœŸæ›´æ–°æˆåŠŸã€‚")
        except Exception as e:
            logger.error(f"å®šæœŸæ›´æ–°å¤±è´¥ï¼š{e}")
            logger.info("å°†ç­‰å¾…1å°æ—¶åå†å°è¯•æ›´æ–°ã€‚")

async def main():
    global url_contents, initial_load_time

    # æ£€æŸ¥ BOT_TOKEN æ˜¯å¦è®¾ç½®
    if not BOT_TOKEN:
        logger.error("æœªè®¾ç½® TELEGRAM_BOT_TOKEN ç¯å¢ƒå˜é‡ã€‚")
        sys.exit(1)

    # ä¸‹è½½å¹¶è§£æå‹ç¼©åŒ…
    logger.info("å¼€å§‹ä¸‹è½½å¹¶è§£æå‹ç¼©åŒ…...")
    try:
        await download_and_parse_archive(ARCHIVE_URL)
    except Exception as e:
        logger.critical(f"æœªèƒ½ä¸‹è½½å‹ç¼©åŒ…ã€‚ç¨‹åºå³å°†ç»ˆæ­¢ã€‚é”™è¯¯ä¿¡æ¯ï¼š{e}")
        sys.exit(1)
    logger.info("æ‰€æœ‰ URL å†…å®¹å·²åŠ è½½ã€‚")

    # åˆå§‹åŒ–æœºå™¨äºº
    application = ApplicationBuilder().token(BOT_TOKEN).build()

    # æ³¨å†Œå‘½ä»¤å¤„ç†å™¨ï¼Œæ”¯æŒåœ¨ç¾¤ç»„ä¸­é@ä½¿ç”¨
    application.add_handler(CommandHandler(["start"], start, filters=filters.COMMAND))
    application.add_handler(CommandHandler(["search", "xsearch"], search, filters=filters.COMMAND))
    application.add_handler(CommandHandler(["regex", "xregex"], regex_search, filters=filters.COMMAND))
    application.add_handler(CommandHandler(["help"], help_command, filters=filters.COMMAND))
    application.add_handler(CommandHandler(["xinfo", "info"], xinfo, filters=filters.COMMAND))
    application.add_handler(CommandHandler(["fetchnow"], fetchnow, filters=filters.COMMAND))
    application.add_handler(CommandHandler(["killnow"], killnow, filters=filters.COMMAND))
    application.add_handler(CommandHandler(["sysinf"], sysinf, filters=filters.COMMAND))

    # æ³¨å†Œé”™è¯¯å¤„ç†å™¨
    application.add_error_handler(error_handler)

    # å¯åŠ¨å®šæœŸæ›´æ–°ä»»åŠ¡
    asyncio.create_task(periodic_update(ARCHIVE_URL))

    # å¯åŠ¨æœºå™¨äºº
    logger.info("æœºå™¨äººå·²å¯åŠ¨ã€‚")
    await application.run_polling()

if __name__ == '__main__':
    asyncio.run(main())
